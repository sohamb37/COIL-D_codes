{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxiAtk1JQg-X",
        "outputId": "c8dbb630-cc3a-4ec5-a08c-b30184f5a789"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount the drive if you are working in google colab\n",
        "\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gL4wqtFxra9Q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import csv\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0j6FFwL1riiF"
      },
      "outputs": [],
      "source": [
        "# Empty lines, trailing and leading spaces are removed\n",
        "\n",
        "def remove_empty_lines(file_path):\n",
        "    \n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            lines = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"\\n\".join(lines) + \"\\n\")\n",
        "\n",
        "        # print(f\"Processed {file_path}: {len(lines)} non-empty lines\") ***************\n",
        "        # logger.debug(f\"Processed {file_path}: {len(lines)} non-empty lines\")\n",
        "        return lines\n",
        "    except Exception as e:\n",
        "        # logger.error(f\"Error processing {file_path}: {str(e)}\")\n",
        "        print(f\"Error processing {file_path}: {str(e)}\")\n",
        "        return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yY4bLqXI--ae"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    Recursively combines all text files from a folder and its subfolders.\n",
        "    Returns tuples of columns for both unreviewed (col1, col2) and reviewed (col1, col3) data.\n",
        "\"\"\"\n",
        "\n",
        "def combine_text_files(input_folder):\n",
        "    \n",
        "    unreviewed_pairs = []  # Will store (col1, col2) pairs\n",
        "    reviewed_pairs = []    # Will store (col1, col3) pairs where available\n",
        "    bad_lines = []\n",
        "\n",
        "    try:\n",
        "        with os.scandir(input_folder) as entries:\n",
        "            for entry in entries:\n",
        "                if entry.is_file() and entry.name.endswith(('.txt', '.tsv')):\n",
        "                    try:\n",
        "                        with open(entry.path, \"rb\") as f:  # Open in binary mode\n",
        "                            for c, byte_line in enumerate(f):\n",
        "                                try:\n",
        "                                    line = byte_line.decode(\"utf-8\").strip()\n",
        "\n",
        "                                    # Skip headers or empty lines\n",
        "                                    if not line or \"Source_Text\" in line:\n",
        "                                        continue\n",
        "\n",
        "                                    # Split the line into columns (handle both tab and comma separators)\n",
        "                                    if '\\t' in line:\n",
        "                                        columns = line.split('\\t')\n",
        "                                    # else:\n",
        "                                    #     columns = line.split(',')\n",
        "\n",
        "                                    # Clean columns\n",
        "                                    columns = [col.strip() for col in columns]\n",
        "\n",
        "                                    # Handle unreviewed pairs (columns 1 and 2)\n",
        "                                    if len(columns) >= 2 and columns[0] and columns[1]:\n",
        "                                        unreviewed_pairs.append((columns[0], columns[1]))\n",
        "\n",
        "                                    # Handle reviewed pairs (columns 1 and 3) when available\n",
        "                                    if len(columns) >= 3 and columns[0] and columns[2]:\n",
        "                                        reviewed_pairs.append((columns[0], columns[2]))\n",
        "\n",
        "                                except UnicodeDecodeError:\n",
        "                                    bad_lines.append({\n",
        "                                        \"file\": entry.name,\n",
        "                                        \"index\": c,\n",
        "                                        \"bytes\": str(byte_line[:20])\n",
        "                                    })\n",
        "                                    # logger.debug(f\"UnicodeDecodeError in file {entry.name} at line {c}\")\n",
        "                                    print(f\"UnicodeDecodeError in file {entry.name} at line {c}\")\n",
        "\n",
        "                        # logger.debug(f\"Processed file {entry.name}\")\n",
        "                        # print(f\"Processed file {entry.name}\")  *******************\n",
        "                    except Exception as e:\n",
        "                        # logger.error(f\"Error reading {entry.path}: {str(e)}\")\n",
        "                        print(f\"Error reading {entry.path}: {str(e)}\")\n",
        "\n",
        "                elif entry.is_dir():\n",
        "                    sub_unreviewed, sub_reviewed, sub_bad_lines = combine_text_files(entry.path)\n",
        "                    unreviewed_pairs.extend(sub_unreviewed)\n",
        "                    reviewed_pairs.extend(sub_reviewed)\n",
        "                    bad_lines.extend(sub_bad_lines)\n",
        "    except Exception as e:\n",
        "        # logger.error(f\"Error processing directory {input_folder}: {str(e)}\")\n",
        "        print(f\"Error processing directory {input_folder}: {str(e)}\")\n",
        "\n",
        "    return unreviewed_pairs, reviewed_pairs, bad_lines\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D_TV5-3hHRTI"
      },
      "outputs": [],
      "source": [
        "\"\"\"Write column pairs to a TSV file.\"\"\"\n",
        "\n",
        "def write_pairs_to_file(pairs, output_file):\n",
        "    \n",
        "    if not pairs:\n",
        "        return 0\n",
        "\n",
        "    try:\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\", newline='') as f:\n",
        "            writer = csv.writer(f, delimiter='\\t')\n",
        "            for pair in pairs:\n",
        "                writer.writerow(pair)\n",
        "        return len(pairs)\n",
        "    except Exception as e:\n",
        "        # logger.error(f\"Error writing to {output_file}: {str(e)}\")\n",
        "        print(f\"Error writing to {output_file}: {str(e)}\")\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2Fw_6Aw-zP3m"
      },
      "outputs": [],
      "source": [
        "\"\"\"Process a directory of translation files organized by language pairs and domains.\"\"\"\n",
        "\n",
        "def process_translation_directory(base_dir, output_dir):\n",
        "    \n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Dictionary to store all processed data\n",
        "    final_dict = {}\n",
        "\n",
        "    # Statistics for reporting\n",
        "    stats = {\n",
        "        \"total_language_pairs\": 0,\n",
        "        \"total_domains\": 0,\n",
        "        \"total_unreviewed_files\": 0,\n",
        "        \"total_reviewed_files\": 0,\n",
        "        \"total_unreviewed_lines\": 0,\n",
        "        \"total_reviewed_lines\": 0\n",
        "    }\n",
        "\n",
        "    # Scan for language pair directories\n",
        "    with os.scandir(base_dir) as languages:\n",
        "        for lang_pair in languages:\n",
        "            if not lang_pair.is_dir():\n",
        "                continue\n",
        "\n",
        "            # logger.info(f\"Processing language pair: {lang_pair.name}\")\n",
        "            # print(f\"Processing language pair: {lang_pair.name}\") *****************\n",
        "            stats[\"total_language_pairs\"] += 1\n",
        "            language_data = []\n",
        "\n",
        "            # Path to the language pair directory\n",
        "            lang_path = os.path.join(base_dir, lang_pair.name)\n",
        "\n",
        "            # Scan for domain directories within the language pair\n",
        "            with os.scandir(lang_path) as domains:\n",
        "                for domain in domains:\n",
        "                    if not domain.is_dir():\n",
        "                        continue\n",
        "\n",
        "                    # logger.info(f\"  Processing domain: {domain.name}\")\n",
        "                    print(f\"  Processing domain: {domain.name}\")\n",
        "                    stats[\"total_domains\"] += 1\n",
        "\n",
        "                    # Combine all files for this domain\n",
        "                    domain_path = os.path.join(lang_path, domain.name)\n",
        "                    # combined_data, bad_lines = combine_text_files(domain_path)\n",
        "                    unreviewed_pairs, reviewed_pairs, bad_lines = combine_text_files(domain_path)\n",
        "\n",
        "                    if not unreviewed_pairs and not reviewed_pairs:\n",
        "                        # logger.warning(f\"  No data found for {lang_pair.name}/{domain.name}\")\n",
        "                        print(f\"  No data found for {lang_pair.name}/{domain.name}\")\n",
        "                        continue\n",
        "\n",
        "\n",
        "                    domain_data = {\n",
        "                        \"domain\": domain.name,\n",
        "                        \"unreviewed\": {\n",
        "                            \"count\": len(unreviewed_pairs),\n",
        "                            \"file_path\": None\n",
        "                        },\n",
        "                        \"reviewed\": {\n",
        "                            \"count\": len(reviewed_pairs),\n",
        "                            \"file_path\": None\n",
        "                        }\n",
        "                    }\n",
        "\n",
        "\n",
        "                    # Create unreviewed file (columns 1 & 2)\n",
        "                    if unreviewed_pairs:\n",
        "                        unreviewed_file = os.path.join(output_dir, f\"unreviewed_{lang_pair.name}_{domain.name}.tsv\")\n",
        "                        unreviewed_count = write_pairs_to_file(unreviewed_pairs, unreviewed_file)\n",
        "\n",
        "                        if unreviewed_count > 0:\n",
        "                            domain_data[\"unreviewed\"][\"file_path\"] = unreviewed_file\n",
        "                            stats[\"total_unreviewed_files\"] += 1\n",
        "                            stats[\"total_unreviewed_lines\"] += unreviewed_count\n",
        "                            # logger.info(f\"  Created unreviewed file with {unreviewed_count} lines: {os.path.basename(unreviewed_file)}\")\n",
        "                            print(f\"  Created unreviewed file with {unreviewed_count} lines: {os.path.basename(unreviewed_file)}\")\n",
        "\n",
        "                    # Create reviewed file (columns 1 & 3) if applicable\n",
        "                    if reviewed_pairs:\n",
        "                        reviewed_file = os.path.join(output_dir, f\"reviewed_{lang_pair.name}_{domain.name}.tsv\")\n",
        "                        reviewed_count = write_pairs_to_file(reviewed_pairs, reviewed_file)\n",
        "\n",
        "                        if reviewed_count > 0:\n",
        "                            domain_data[\"reviewed\"][\"file_path\"] = reviewed_file\n",
        "                            stats[\"total_reviewed_files\"] += 1\n",
        "                            stats[\"total_reviewed_lines\"] += reviewed_count\n",
        "                            # logger.info(f\"  Created reviewed file with {reviewed_count} lines: {os.path.basename(reviewed_file)}\")\n",
        "                            print(f\"  Created reviewed file with {reviewed_count} lines: {os.path.basename(reviewed_file)}\")\n",
        "\n",
        "                    # Log bad lines if any\n",
        "                    if bad_lines:\n",
        "                        # logger.warning(f\"  Found {len(bad_lines)} problematic lines in {domain.name}\")\n",
        "                        print(f\"  Found {len(bad_lines)} problematic lines in {domain.name}\")\n",
        "\n",
        "                    # Store domain data in our dictionary\n",
        "                    language_data.append(domain_data)\n",
        "\n",
        "            # Store all data for this language pair\n",
        "            final_dict[lang_pair.name] = language_data\n",
        "\n",
        "    # Print summary statistics\n",
        "    # logger.info(\"\\nProcessing Summary:\")\n",
        "    # logger.info(f\"Total language pairs processed: {stats['total_language_pairs']}\")\n",
        "    # logger.info(f\"Total domains processed: {stats['total_domains']}\")\n",
        "    # logger.info(f\"Total unreviewed files created: {stats['total_unreviewed_files']} with {stats['total_unreviewed_lines']} lines\")\n",
        "    # logger.info(f\"Total reviewed files created: {stats['total_reviewed_files']} with {stats['total_reviewed_lines']} lines\")\n",
        "\n",
        "    print(\"\\nProcessing Summary:\")\n",
        "    print(f\"Total language pairs processed: {stats['total_language_pairs']}\")\n",
        "    print(f\"Total domains processed: {stats['total_domains']}\")\n",
        "    print(f\"Total unreviewed files created: {stats['total_unreviewed_files']} with {stats['total_unreviewed_lines']} lines\")\n",
        "    print(f\"Total reviewed files created: {stats['total_reviewed_files']} with {stats['total_reviewed_lines']} lines\")\n",
        "\n",
        "    return final_dict, stats\n",
        "\n",
        "    #                 # Create output file name: LANG1-LANG2_DOMAIN.txt\n",
        "    #                 output_file = os.path.join(output_dir, f\"{lang_pair.name}_{domain.name}.txt\")\n",
        "\n",
        "    #                 # Write combined data to file\n",
        "    #                 with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    #                     f.write(\"\\n\".join(combined_data) + \"\\n\")\n",
        "\n",
        "    #                 # Remove empty lines and get clean data\n",
        "    #                 clean_data = remove_empty_lines(output_file)\n",
        "\n",
        "    #                 # Update statistics\n",
        "    #                 stats[\"total_files_created\"] += 1\n",
        "    #                 stats[\"total_lines_processed\"] += len(clean_data)\n",
        "\n",
        "    #                 # Log results\n",
        "    #                 logger.info(f\"  Domain: {domain.name} - Raw lines: {len(combined_data)} - \"\n",
        "    #                            f\"Clean lines: {len(clean_data)}\")\n",
        "\n",
        "    #                 # Store data in our dictionary\n",
        "    #                 language_data.append({\n",
        "    #                     \"domain\": domain.name,\n",
        "    #                     \"data\": clean_data,\n",
        "    #                     \"file_path\": output_file\n",
        "    #                 })\n",
        "\n",
        "    #                 # Log bad lines if any\n",
        "    #                 if bad_lines:\n",
        "    #                     logger.warning(f\"  Found {len(bad_lines)} problematic lines in {domain.name}\")\n",
        "\n",
        "    #         # Store all data for this language pair\n",
        "    #         final_dict[lang_pair.name] = language_data\n",
        "\n",
        "    # # Print summary statistics\n",
        "    # logger.info(\"\\nProcessing Summary:\")\n",
        "    # logger.info(f\"Total language pairs processed: {stats['total_language_pairs']}\")\n",
        "    # logger.info(f\"Total domains processed: {stats['total_domains']}\")\n",
        "    # logger.info(f\"Total output files created: {stats['total_files_created']}\")\n",
        "    # logger.info(f\"Total lines processed: {stats['total_lines_processed']}\")\n",
        "\n",
        "    # return final_dict, stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Make changes to the path in the block below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuHKvxti29QY",
        "outputId": "10d7e701-3862-48da-88d0-86c3c8b411a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting processing of translation files\n",
            "Input directory: /content/drive/MyDrive/COIL-D/Test/\n",
            "Output directory: /content/output_test/\n",
            "  Processing domain: HLT\n",
            "  Created unreviewed file with 2289 lines: unreviewed_HIN-MAI_HLT.tsv\n",
            "  Created reviewed file with 2290 lines: reviewed_HIN-MAI_HLT.tsv\n",
            "  Processing domain: JUD\n",
            "  Created unreviewed file with 2550 lines: unreviewed_HIN-MAI_JUD.tsv\n",
            "  Created reviewed file with 2530 lines: reviewed_HIN-MAI_JUD.tsv\n",
            "  Processing domain: GOV\n",
            "  Created unreviewed file with 18429 lines: unreviewed_HIN-MAI_GOV.tsv\n",
            "  Created reviewed file with 18434 lines: reviewed_HIN-MAI_GOV.tsv\n",
            "  Processing domain: EDU\n",
            "  Created unreviewed file with 6913 lines: unreviewed_HIN-MAI_EDU.tsv\n",
            "  Created reviewed file with 6901 lines: reviewed_HIN-MAI_EDU.tsv\n",
            "  Processing domain: AGRI\n",
            "  Created unreviewed file with 3002 lines: unreviewed_HIN-MAI_AGRI.tsv\n",
            "  Created reviewed file with 2982 lines: reviewed_HIN-MAI_AGRI.tsv\n",
            "\n",
            "Processing Summary:\n",
            "Total language pairs processed: 1\n",
            "Total domains processed: 5\n",
            "Total unreviewed files created: 5 with 33183 lines\n",
            "Total reviewed files created: 5 with 33137 lines\n",
            "Processing complete!\n"
          ]
        }
      ],
      "source": [
        "# Configuration - adjust these paths as needed\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    INPUT_FOLDER = \"/content/drive/MyDrive/COIL-D/Test/\"\n",
        "    OUTPUT_FOLDER = \"/content/output_test/\"\n",
        "\n",
        "    # logger.info(f\"Starting processing of translation files\")\n",
        "    # logger.info(f\"Input directory: {INPUT_FOLDER}\")\n",
        "    # logger.info(f\"Output directory: {OUTPUT_FOLDER}\")\n",
        "\n",
        "    print(f\"Starting processing of translation files\")\n",
        "    print(f\"Input directory: {INPUT_FOLDER}\")\n",
        "    print(f\"Output directory: {OUTPUT_FOLDER}\")\n",
        "\n",
        "    # Process all files\n",
        "    result_dict, stats = process_translation_directory(INPUT_FOLDER, OUTPUT_FOLDER)\n",
        "\n",
        "    # logger.info(\"Processing complete!\")\n",
        "    print(\"Processing complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Download the folder if it is in google colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZJ1xszioQbJx",
        "outputId": "62ecade4-730b-4f1d-be96-095298195cec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folder '/content/output_test' zipped successfully to 'output_test.zip'\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_f17bf1c9-f5f1-4752-90c0-e532a1fed027\", \"output_test.zip\", 6424991)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# prompt: zip and download a folder\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "def zip_and_download_folder(folder_path, zip_filename):\n",
        "    \"\"\"Zips a folder and downloads the zip file.\"\"\"\n",
        "    try:\n",
        "        shutil.make_archive(zip_filename, 'zip', folder_path)\n",
        "        print(f\"Folder '{folder_path}' zipped successfully to '{zip_filename}.zip'\")\n",
        "        # For downloading in colab environment\n",
        "        from google.colab import files\n",
        "        files.download(f'{zip_filename}.zip')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Folder '{folder_path}' not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Example usage (replace with your folder path and desired zip file name)\n",
        "zip_and_download_folder(\"/content/output_test\", \"output_test\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
