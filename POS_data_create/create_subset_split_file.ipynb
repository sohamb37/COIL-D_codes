{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66ac1939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-empty rows per column:\n",
      "source_HIN    294\n",
      "HIN-ASM       294\n",
      "HIN-BAN       294\n",
      "HIN-BRX         0\n",
      "HIN-DOI       294\n",
      "HIN-GOM         0\n",
      "HIN-GUJ       294\n",
      "HIN-KAS       294\n",
      "HIN-MAI       294\n",
      "HIN-MAR       294\n",
      "HIN-MNI       294\n",
      "HIN-NPI       294\n",
      "HIN-ODI       294\n",
      "HIN-PAN       294\n",
      "HIN-SAT         1\n",
      "HIN-SND         0\n",
      "HIN-TEL       294\n",
      "HIN-URD         1\n",
      "TAM-KAN         1\n",
      "TAM-MAL         1\n",
      "TAM-TEL         1\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_112328/1316178750.py:9: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({\"\": pd.NA})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load TSV (adjust file path)\n",
    "tsv_path = \"/home/soham37/python/POS_data_create/EDU/merged_edu_2.tsv\"\n",
    "df = pd.read_csv(tsv_path, sep=\"\\t\", dtype=str)  # read as strings to handle empties uniformly\n",
    "\n",
    "# Normalize empties: strip whitespace and set empty strings to NaN\n",
    "df = df.apply(lambda col: col.str.strip() if col.dtype == \"object\" else col)\n",
    "df = df.replace({\"\": pd.NA})\n",
    "\n",
    "# Count non-empty per column (including Source if needed)\n",
    "non_empty_counts = df.notna().sum()\n",
    "\n",
    "print(\"Non-empty rows per column:\")\n",
    "print(non_empty_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f54c35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final subset size: 290\n",
      "Non-empty rows per column in subset:\n",
      "source_HIN    290\n",
      "HIN-ASM       290\n",
      "HIN-BAN       290\n",
      "HIN-BRX         0\n",
      "HIN-DOI       290\n",
      "HIN-GOM         0\n",
      "HIN-GUJ       290\n",
      "HIN-KAS       290\n",
      "HIN-MAI       290\n",
      "HIN-MAR       290\n",
      "HIN-MNI       290\n",
      "HIN-NPI       290\n",
      "HIN-ODI       290\n",
      "HIN-PAN       290\n",
      "HIN-SAT         1\n",
      "HIN-SND         0\n",
      "HIN-TEL       290\n",
      "HIN-URD         1\n",
      "TAM-KAN         1\n",
      "TAM-MAL         1\n",
      "TAM-TEL         1\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_112328/290598085.py:23: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  selected_idx = selected_idx.append(pick)\n"
     ]
    }
   ],
   "source": [
    "target_size = 290\n",
    "source_col = df.columns[0]\n",
    "candidate_cols = [c for c in df.columns if c != source_col]\n",
    "\n",
    "# Compute availability mask for each candidate column\n",
    "non_empty_masks = {c: df[c].notna() for c in candidate_cols}\n",
    "counts = {c: int(mask.sum()) for c, mask in non_empty_masks.items()}\n",
    "\n",
    "# 1) Order columns by scarcity (ascending)\n",
    "ordered_cols = sorted(candidate_cols, key=lambda c: counts[c])\n",
    "\n",
    "# 2) Seed selection with rows that are non-empty for the scarcest column(s)\n",
    "selected_idx = pd.Index([])\n",
    "used_mask = pd.Series(False, index=df.index)\n",
    "\n",
    "for col in ordered_cols:\n",
    "    if len(selected_idx) >= target_size:\n",
    "        break\n",
    "    # rows available for this col that aren't already picked\n",
    "    avail = non_empty_masks[col] & (~used_mask)\n",
    "    needed = target_size - len(selected_idx)\n",
    "    pick = df.index[avail][:needed]  # stable selection\n",
    "    selected_idx = selected_idx.append(pick)\n",
    "    used_mask.loc[pick] = True\n",
    "\n",
    "# 3) If still short of 600, fill with rows that maximize total non-empties across all columns\n",
    "if len(selected_idx) < target_size:\n",
    "    remaining = df.index[~used_mask]\n",
    "    # Score rows by how many non-empty candidate entries they have\n",
    "    row_scores = df.loc[remaining, candidate_cols].notna().sum(axis=1)\n",
    "    # Choose highest-scoring rows until we hit 600\n",
    "    needed = target_size - len(selected_idx)\n",
    "    filler = row_scores.sort_values(ascending=False).index[:needed]\n",
    "    selected_idx = selected_idx.append(filler)\n",
    "\n",
    "subset = df.loc[selected_idx].copy()\n",
    "\n",
    "# Optional: ensure uniqueness by Source and keep first occurrence\n",
    "subset = subset.drop_duplicates(subset=[source_col], keep=\"first\")\n",
    "\n",
    "# If duplicates removal made it smaller than 600, top up again (optional)\n",
    "if len(subset) > target_size:\n",
    "    subset = subset.iloc[:target_size]\n",
    "elif len(subset) < target_size:\n",
    "    # Try to top up with remaining rows (lower score) without hurting scarcest columns\n",
    "    remaining2 = df.index.difference(subset.index)\n",
    "    row_scores2 = df.loc[remaining2, candidate_cols].notna().sum(axis=1)\n",
    "    topup = row_scores2.sort_values(ascending=False).index[:(target_size - len(subset))]\n",
    "    subset = pd.concat([subset, df.loc[topup]]).drop_duplicates(subset=[source_col], keep=\"first\").iloc[:target_size]\n",
    "\n",
    "print(f\"Final subset size: {len(subset)}\")\n",
    "\n",
    "# Check non-empty counts in the subset\n",
    "subset_counts = subset.notna().sum()\n",
    "print(\"Non-empty rows per column in subset:\")\n",
    "print(subset_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebde0be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save counts\n",
    "counts_df = pd.DataFrame({\n",
    "    \"column\": df.columns,\n",
    "    \"non_empty_full\": [int(df[c].notna().sum()) for c in df.columns],\n",
    "    \"non_empty_subset\": [int(subset[c].notna().sum()) for c in df.columns]\n",
    "})\n",
    "counts_df.to_csv(\"/home/soham37/python/POS_data_create/EDU/sent_count_in_290_sample.csv\", index=False)\n",
    "\n",
    "# Save subset\n",
    "subset.to_csv(\"/home/soham37/python/POS_data_create/EDU/subset_290.tsv\", sep=\"\\t\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe38a0e",
   "metadata": {},
   "source": [
    "# Create Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/soham37/Downloads/Create_data_pos - subset_edu_1000.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b26b894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f9d57ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "949b393f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['source_HIN', 'HIN-ASM', 'HIN-BAN', 'HIN-DOI', 'HIN-GOM', 'HIN-GUJ',\n",
       "       'HIN-KAS', 'HIN-MAI', 'HIN-MAR', 'HIN-MNI', 'HIN-NPI', 'HIN-ODI',\n",
       "       'HIN-PAN', 'HIN-SAT', 'HIN-SND', 'HIN-TEL', 'HIN-URD', 'TAM-KAN',\n",
       "       'TAM-MAL', 'TAM-TEL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a785cefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"source_HIN\", \"HIN-BAN\", \"HIN-MAI\", \"HIN-ODI\", \"HIN-SAT\"]] #df for iitp language pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "117ff6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Files created successfully in: /home/soham37/python/POS_data_create/EDU/IITP\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "\n",
    "# Example: Load your dataframe\n",
    "df = df.fillna(\"\")  # replace NaN with empty strings\n",
    "\n",
    "# Directory to save output files\n",
    "out_dir = \"/home/soham37/python/POS_data_create/EDU/IITP\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Assign unique IDs for source and translations\n",
    "df = df.reset_index(drop=True)\n",
    "df[\"source_id\"] = [f\"SRC_{i+1}\" for i in range(len(df))]\n",
    "\n",
    "# Language pair columns (skip 'source_id' and 'source')\n",
    "lang_pairs = [col for col in df.columns if col not in [\"source_HIN\", \"source_id\"]]\n",
    "\n",
    "for lp in lang_pairs:\n",
    "    # Create per-language subfolder\n",
    "    lp_dir = os.path.join(out_dir, lp)\n",
    "    os.makedirs(lp_dir, exist_ok=True)\n",
    "\n",
    "    # Unique translation IDs\n",
    "    df[f\"{lp}_id\"] = [\n",
    "        f\"{lp}_{i+1}\" if val.strip() != \"\" else \"\"\n",
    "        for i, val in enumerate(df[lp])\n",
    "    ]\n",
    "\n",
    "    # Non-empty rows\n",
    "    non_empty_df = df[df[lp].str.strip() != \"\"]\n",
    "    empty_df = df[df[lp].str.strip() == \"\"]\n",
    "\n",
    "    # Save empty rows: only source_id + source\n",
    "    empty_file = os.path.join(lp_dir, f\"{lp}_empty.txt\")\n",
    "    empty_df[[\"source_id\", \"source_HIN\"]].to_csv(empty_file, index=False, sep=\"\\t\", header=False)\n",
    "\n",
    "    # Save non-empty rows: source_id, source, translation_id, translation\n",
    "    non_empty_file = os.path.join(lp_dir, f\"{lp}_non_empty.txt\")\n",
    "    non_empty_df[[\"source_id\", \"source_HIN\", f\"{lp}_id\", lp]].to_csv(non_empty_file, index=False, sep=\"\\t\", header=False)\n",
    "\n",
    "    # Split non-empty into chunks of 30\n",
    "    num_chunks = math.ceil(len(non_empty_df) / 30)\n",
    "    for i in range(num_chunks):\n",
    "        chunk = non_empty_df.iloc[i*30 : (i+1)*30]        \n",
    "        chunk_file = os.path.join(lp_dir, f\"{lp}_non_empty_part_{i+1}.txt\")\n",
    "        # chunk[[\"source_id\", \"source\", f\"{lp}_id\", lp]].to_csv(chunk_file, index=False, sep=\"\\t\", header=False)\n",
    "        chunk[lp].to_csv(chunk_file, index=False, header=False) # save only the translations        \n",
    "\n",
    "print(\"✅ Files created successfully in:\", out_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d44d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
